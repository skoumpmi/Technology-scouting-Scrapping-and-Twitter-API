# -*- coding: utf-8 -*-
"""text_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oc2s5UiRoNvBFrrPlxKqVJLucUjMfBi_
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Run the following code to print progress-bar
#!pip install --force https://github.com/chengs/tqdm/archive/colab.zip

# Commented out IPython magic to ensure Python compatibility.
#!pip install -q tf-nightly-2.0-preview
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir "drive/My Drive/Colab_Notebooks/text_cnn/summaries"

LOCAL_FOLDER='drive/My Drive/Colab_Notebooks/text_cnn'
import sys
sys.path.append(LOCAL_FOLDER)
from time import gmtime, strftime
from tqdm.auto import tqdm, trange
from tqdm import tqdm_notebook
from torchtext import datasets
from sklearn import metrics
from torchtext import vocab
from torchtext import data
from CNN_txt import CNN2d
from copy import deepcopy
from logger import Logger
from utils import *
import numpy as np
import mydatasets
import argparse
import logging
import random
import numpy
import time
import json
import os

tqdm.pandas(desc='Progress', position=0, leave=True)
torch.backends.cudnn.deterministic = True
logger = logging.getLogger()

SEED = 1234
torch.manual_seed(SEED)

def train(model, model_param, iterator, optimizer, criterion):

    epoch_loss = 0
    epoch_acc = 0
    y_true = []
    y_score = []

    model.train()
    for X, y in tqdm(iterator, desc="Trainning Iteration", leave=True, position=0):

        optimizer.zero_grad()
        predictions = model(X)
        # CrossEntropyLoss expects the input to be [batch size, n classes] and the label to be [batch size].
        loss = criterion(predictions, y.long())
        acc, max_preds, max_preds_index = categorical_accuracy(predictions, y.long(), device=DEVICE)
        loss.backward()
        nn.utils.clip_grad_norm_(model_param, max_norm=3)
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

        y_true.extend(y.cpu().data.numpy().astype(np.float32))
        y_score.extend(max_preds_index.cpu().data.numpy().astype(np.float32))

    roc_auc = multiclass_roc_score(y_true, y_score, average="macro")

    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc, model, optimizer

def evaluate(model, iterator, criterion):

    if not iterator:
        return None, None, None

    epoch_loss = 0
    epoch_acc = 0
    y_true = []
    y_score = []

    model.eval()
    with torch.no_grad():

        for X, y in tqdm(iterator, desc="Evaluate Iteration", leave=True, position=0):

            predictions = model(X)#.squeeze(1)
            loss = criterion(predictions, y.long())
            acc, max_preds, max_preds_index = categorical_accuracy(predictions, y.long(), device=DEVICE)

            epoch_loss += loss.item()
            epoch_acc += acc.item()

            y_true.extend(y.cpu().data.numpy().astype(np.float32))
            y_score.extend(max_preds_index.cpu().data.numpy().astype(np.float32))

    # Compute fpr, tpr, thresholds and roc auc
    #fpr, tpr, thresholds = roc_curve(y_true, y_score)
    roc_auc = multiclass_roc_score(y_true, y_score, average="macro")
    # roc_auc = roc_auc_score(y_true, y_score)

    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc

def fit_model(model, model_param, train_iterator, test_iterator,
                valid_iterator, optimizer, criterion, start_epoch=1, save_model=False):

    # Set the logger
    train_loss_logger = Logger(os.path.join(LOG_FOLDER, "train_logs"))
    train_acc_logger = Logger(os.path.join(LOG_FOLDER, "train_acc"))
    train_auc_score_logger = Logger(os.path.join(LOG_FOLDER, "train_auc_score"))

    if valid_iterator:
        val_loss_logger = Logger(os.path.join(LOG_FOLDER, "val_logs"))
        val_acc_logger = Logger(os.path.join(LOG_FOLDER, "val_acc"))
        val_auc_score_logger = Logger(os.path.join(LOG_FOLDER, "val_auc_score"))

    if test_iterator:
        test_loss_logger = Logger(os.path.join(LOG_FOLDER, "test_logs"))
        test_acc_logger = Logger(os.path.join(LOG_FOLDER, "test_acc"))
        test_auc_score_logger = Logger(os.path.join(LOG_FOLDER, "test_auc_score"))

    for epoch in tqdm(range(start_epoch, int(N_EPOCHS)), desc="Epoch Iteration", leave=True, position=0):

        start_time = time.time()
        train_loss, train_acc, train_roc_auc, model, optimizer = train(model,
                                                                       model_param,
                                                                       train_iterator,
                                                                       optimizer,
                                                                       criterion)

        save_cnn_checkpoint(model=model,
                            optimizer=optimizer,
                            epoch=epoch,
                            output_folder=OUTPUT_FOLDER,
                            config_name=CONFIG_NAME,
                            model_name=MODEL_NAME,
                            save_model=True)

        valid_loss, valid_acc, valid_roc_auc  = evaluate(model, valid_iterator, criterion)
        valid_loss, valid_acc, valid_roc_auc  = evaluate(model, valid_iterator, criterion)
        test_loss, test_acc, test_roc_auc = evaluate(model, test_iterator, criterion)

        end_time = time.time()

        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')

        # ============ TensorBoard logging ============#
        # Log the scalar values
        train_loss_info = {'loss': train_loss}
        train_acc_info = {'accuracy': train_acc*100}
        train_auc_info = {'AUC': train_roc_auc}

        for tag, value in train_acc_info.items(): train_acc_logger.scalar_summary(tag, value, step=epoch)
        for tag, value in train_loss_info.items(): train_loss_logger.scalar_summary(tag, value, step=epoch)
        for tag, value in train_auc_info.items(): train_auc_score_logger.scalar_summary(tag, value, step=epoch)
        print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train AUC: {train_roc_auc :.4f}%')

        if valid_iterator:
            val_loss_info = {'loss': valid_loss}
            val_roc_auc_info = {'AUC': valid_roc_auc}
            val_acc_info = {'accuracy': valid_acc * 100}
            for tag, value in val_acc_info.items(): val_acc_logger.scalar_summary(tag, value, step=epoch)
            for tag, value in val_loss_info.items(): val_loss_logger.scalar_summary(tag, value, step=epoch)
            for tag, value in val_roc_auc_info.items(): val_auc_score_logger.scalar_summary(tag, value, step=epoch)
            print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')
            print(f'\t Val. AUC: {valid_roc_auc:.3f}')

        if test_iterator:
            test_loss_info = {'loss': test_loss}
            test_acc_info = {'accuracy': test_acc * 100}
            test_roc_auc_info = {'AUC': test_roc_auc}
            for tag, value in test_loss_info.items(): test_loss_logger.scalar_summary(tag, value, step=epoch)
            for tag, value in test_roc_auc_info.items(): test_auc_score_logger.scalar_summary(tag, value, step=epoch)
            for tag, value in test_acc_info.items(): test_acc_logger.scalar_summary(tag, value, step=epoch)
            print(f'\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc * 100:.2f}%')
            print(f'\t Test. AUC: {test_roc_auc:.3f}')

def predict_sentiment(model, sentence, min_len=5, model_name=None):

    model.load_state_dict(torch.load(model_name))
    model.eval()
    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]
    if len(tokenized) < min_len:
        tokenized += ['<pad>'] * (min_len - len(tokenized))
    indexed = [TEXT.vocab.stoi[t] for t in tokenized]
    tensor = torch.LongTensor(indexed).to(DEVICE)
    tensor = tensor.unsqueeze(1)
    prediction = torch.sigmoid(model(tensor))

    return prediction.item()

def main():

    LEARNING_RATE = 0.001

    BATCH_SIZES = (40, 256, 256)

    train_dl, valid_dl, test_dl, criterion, TEXT, LABEL = load_cnn_data(task_name=TASK_NAME,
                                                                cache_folder=CACHE_DIR,
                                                                device=DEVICE,
                                                                batch_sizes=BATCH_SIZES,
                                                                embeddings_file=EMBEDDINGS_FILE,
                                                                embeddings_path=os.path.join(CACHE_DIR, 'embeddings'))
    if not FLAGS.restore_point:
        print('Initialize Model...')
        start_epoch=1
        INPUT_DIM = len(TEXT.vocab)
        MULTI_CHANNEL = False
        EMBEDDING_DIM = 300
        N_FILTERS = 100
        FILTER_SIZES = [3, 4, 5]
        OUTPUT_DIM = len(LABEL.vocab)
        DROPOUT = 0.5
        PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]
        model = CNN2d(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM,
                              n_filters=N_FILTERS, filter_sizes=FILTER_SIZES,
                              multi_channel=MULTI_CHANNEL, output_dim=OUTPUT_DIM,
                              dropout=DROPOUT, pad_idx=PAD_IDX)

        pretrained_embeddings = TEXT.vocab.vectors
        model.embedding.weight.data.copy_(pretrained_embeddings)

        if MULTI_CHANNEL:
            model.word_emb_multi.weight.data.copy_(pretrained_embeddings)

        UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]
        model.embedding.weight.data[UNK_IDX] = torch.FloatTensor(EMBEDDING_DIM).uniform_(-0.0025, 0.0025)  # np.random.uniform(-0.01, 0.01, EMBEDDING_DIM).astype("float32") #torch.zeros(EMBEDDING_DIM)
        model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

        parameters = filter(lambda p: p.requires_grad, model.parameters())
        optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.8)

        model = model.to(DEVICE)

    else:
        model, optimizer, parameters, start_epoch = load_cnn_model(device=DEVICE,
                                                               restore_point=FLAGS.restore_point,
                                                               output_folder=OUTPUT_FOLDER,
                                                               config_name=CONFIG_NAME,
                                                               model_name=MODEL_NAME,
                                                               gpu_enabled=FLAGS.gpu_mode)

    criterion = criterion.to(DEVICE)

    print(optimizer.state_dict)
    print(f'The model has {count_parameters(model):,} trainable parameters')

    ## Train ###
    fit_model(model=model,
                start_epoch=start_epoch,
                model_param=parameters,
                train_iterator=train_dl,
                test_iterator=test_dl,
                valid_iterator=valid_dl,
                optimizer=optimizer,
                criterion=criterion,
                save_model=FLAGS.save_model)

if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    ### Required parameters
    parser.add_argument("--num_train_epochs", default=100.0, required=True, type=float,
                        help="Total number of training epochs to perform.")
    parser.add_argument("--task_name", default=None, type=str, required=True,
                        help="The name of the task ('yelp' or 'sst-2' or 'sst-5' or 'imdb') to train. ")
    parser.add_argument("--main_path", type=str, default='./', required=False,
                        help="The main path where all files will be stored.")
    parser.add_argument("--save_model", type=str2bool, default=True, required=True,
                        help="Make or not checkpoints.")
    parser.add_argument("--restore_point", type=str, default='', required=False,
                        help="Continue model execution.")
    parser.add_argument("--gpu_mode", type=str2bool, default=True, required=False,
                        help="To run in gpu or not.")
    parser.add_argument("--embeddings_file", type=str, default="GoogleNews-vectors-negative300.bin.gz", required=False,
                        help="The filename of the pre-computed embeddings.")

    FLAGS = parser.parse_args()

    logger.info("num_train_epochs = %d", FLAGS.num_train_epochs)
    logger.info("task_name = %s", FLAGS.task_name)
    logger.info("main_path = %s", FLAGS.main_path)
    logger.info("save_model = %s", FLAGS.save_model)
    logger.info("restore_point = %s", FLAGS.restore_point)

    N_EPOCHS            =   FLAGS.num_train_epochs
    TASK_NAME           =   FLAGS.task_name
    EMBEDDINGS_FILE     =   FLAGS.embeddings_file

    CACHE_DIR           =   os.path.join(FLAGS.main_path, "cache/")
    OUTPUT_FOLDER       =   os.path.join(FLAGS.main_path, 'cnn_outputs', TASK_NAME)
    LOG_FOLDER          =   os.path.join(OUTPUT_FOLDER, "summaries")

    if FLAGS.gpu_mode:
        if torch.cuda.is_available():
            DEVICE = torch.device("cuda")
        else:
            DEVICE = torch.device("cpu")
    else:
        DEVICE = torch.device("cpu")

    CONFIG_NAME         =   "config.json"
    MODEL_NAME          =   "pytorch_model.bin"

    print('--------------------------------------')
    print("OUTPUT_FOLDER {0}".format(OUTPUT_FOLDER))
    print("CACHE_DIR {0}".format(CACHE_DIR))
    print("LOG_FOLDER {0}".format(LOG_FOLDER))
    print('--------------------------------------')

    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

    if not os.path.exists(LOG_FOLDER):
        os.makedirs(LOG_FOLDER)

    main()